\chapter{Integration mit der Umgebung}

Eine Webapplikation sollte niemals isoliert betrachtet werden. Auch wenn die vorgestellten Kopmonenten perfekt umgesetzt werden, ergibt sich aus der Interaktion zwischen der theoretischen Webapplikation und der realen Umgebung immer ein Gefahren- bzw. Verbesserungspotential.

Hierbei kann es sich z. B. um nicht-funktionale Elemente wie Logging handeln, oder aber auch um Aspekte wie Prgrammiersprach-inheränte Muster, die einen negativen Einfluss auf die Sicherheit der Webapplikation besitzen können.

\section{Using Components with Known Vulnerabilities}

Die Verwendung vorhandener (und gewarteter) externer Komponenenten wie Bibliotheken oder Framworks besitzt sicherheitstechnisch viele Vorteile: man kann von Fehlern anderen lernen (und vermeidet diese selbst) bzw. muss das Rad nicht neu erfinden.

Damit wird allerdings auch der Nachteil eingekauft, dass eine Verwundbarkeit innerhalb einer integrierten externen Komponente automatisch auch eine Verwundbarkeit der eigenen Applikation impliziert. Daher müssen aktiv und regelmäßig verwendete Komponenten auf bekannte Schwachstellen hin überprüft, und ggf. die betroffenen Komponenten aktualisiert werden.

Um den Aufwand dieser Überprüfungen zu reduzieren und damit optimaler-weise deren Frequenz zu erhöhen gibt es automatisierte tools wie den OWASP Dependency-Check\footnote{\url{https://jeremylong.github.io/DependencyCheck/analyzers/index.html}}. Diese analysieren automatisch Projekte auf Dependencies (z. B. über Ruby Gemfiles, NPM package-lock.json Files, Maven Projektbeschreibungen), extrahieren automatisch dependencies und korrelieren diese mit öffentlichen Verwundbarkeitsdatenbanken. Wird hier nun eine potentiell anwendbare Schwachstelle gefunden, wird der Entwickler via Email oder Slack notifiziert.

\section{Insufficient Logging and Monitoring}

Diese Schwachstelle wurde im Jahre 2017 neu bei den OWASP Top 10 aufgenommen. Es handelt sich hierbei weniger um eine Schwachstelle während der Exekution, sondern eher um die Schaffung der Möglichkeit nach einem Angriff aufgrund der vorhandenen Log-Dateien das Vorgehen des Angreifers und die betroffenen Daten zu erkennen.

Folgende groben Anforderungen an das Log-System werden gestellt:

\begin{itemize}
	\item Es muss mit verteilten Applikationen umgehen können. Eine Webapplikation ist potentiell auf mehrere Computersysteme verteilt (Webserver, Applikationsserver, Datenbankserver). Die Logdaten der gesamten Systeme sollten an einer Stelle aggregiert werden.
	\item Es muss die Integrität der Logdaten schützen: ein Angreifer sollte keine Möglichkeit besitzen, die geloggten Daten zu beeinflussen. Würden z. B. Logdaten direkt am Webserver gespeichert werden, könnte ein Angreifer der den Webserver gehackt hat, ebenso die Logdaten modifizieren. Dies impliziert, dass der Log-Server über eine genau definierte API erreichbar sein sollte.
	\item Es muss die Vertraulichkeit der Daten schützen. Da der Logserver Detailinformationen über betriebliche Abläufe speichert, müssen diese Daten mindestens ebenso sicher wie die ursprünglichen Daten gespeichert werden.
	\item Das Log-System muss Möglichkeiten zur nachträglichen Auswertung der gesammelten Daten bieten. Bonuspunkte, wenn man ein automatisiertes Monitoring mit dem Log-System betreiben kann.
\end{itemize}

Die jeweiligen loggenden Systeme sollten alle sicherheitsrelevanten Events (z. B. Input Validation Fehler, Authentication Fehler, Authorization Fehler, Applikations-Fehler) an das zentrale Log-System schicken. Diese Daten sollen um Business Process Events angereichtert werden. Diese dienen dazu, relevante geschäfts-relevante Prozesse und Ereignisse mit den Sicherheits-Events zu korrelieren. Weitere Datenquellen sind z. B. Anti-Automatisierungssysteme, welche Brute-Force Angriffe erkennen, Datenverarbeitungssysteme (können auch Batch-Systeme sein, die z. B. einen Daten-Export oder Backups ausführen) und alle direkt und indirekt involvierten Services, wie z. B. Mailserver, Datenbankserver, Backupdienste. Falls vorhanden, sollten die Loginformationen sicherheitsrelevanter Komponenten (HIDS, NIDS, WAFs) auf jeden Fall inkludiert werden.

Bei dem Loggen sollte darauf geachtet werden, dass, wenn möglich, standardisierte Log-Formate wie CEF oder LEEF verwendet werden. Dadurch wird das Konvertieren der jeweiligen Datenquellen auf ein gemeinsames Format vermieden.

Welche Daten sollten pro Event erfasst werden?

\begin{itemize}
	\item Wann hat sich der Vorfall ereignet? Bei einer verteilten Applikation sollte hier darauf geachtet werden, dass Timestamps die Zeitzone beinhalten (und auch auf Zeitumstellungen achten). Grundlage für das temporale korrelieren von Events ist es, dass alle beteiligten Server eine idente Systemzeit besitzen (z. B. durch die Verwendung von ntp).
	\item Wo ist das Event passiert? Hierfür können Systemnamen, Servicenamen, Containernamen oder Applikationsnamen verwendet werden.
	\item Für welchen Benutzer ist das Event passiert? Hier können Systembenutzer (mit denen das Service läuft) oder feingranular der gerade eingeloggte Benutzer protokolliert werden.
	\item Was ist passiert? Dies wird immer applikations- und event-spezifisch sein. Viele Systeme verwenden zumindest eine idente Klassifizierung der Wichtigkeit des Events.
\end{itemize}

Die Verwendung von personenbezogenen Daten kann das Logging verkomplizieren. Ein Unternehmen sollte klare Regeln erstellen, welche Daten geloggt werden und, falls notwendig, Anonymisierung oder Pseudonymisierung verwenden um sensible Daten zu maskieren. Ein ähnliches Problem tritt auf, wenn Log-Informationen zwischen Unternehmen geteilt werden sollte (z. B. im Zuge eines Informations-Lagebilds). Da diese Daten unter anderem personen-bezogene Informationen als auch Betriebsgeheimnisse inkludieren können, wird davon meistens abgesehen.

Die erfassten Daten sollten im Zuge einer Auswertung verwendet werden. Hier werden häufig "normale" Texteditoren in Verbindung mit regulären Ausdrücken verwendet. Fortgeschrittenere Lösungen wären ELK-Stacks, Kibana und Logstash und z. B. Splunk.

In einem ähnlichem Umfeld arbeiten SIEM-Systeme (Security Information and Event Management). Diese werden zumeist als weiterer Schritt nach Log-Management angesehen. Zusätzlich zum Log-Management wird zumeist auch Security Event Management (real-time monitoring), Security Information Management (long-term storage of security events) und Security Event Correlattion durchgeführt.

\chapter{DevOps und Tooling}

DevOps ist eine neuere Strömung die versucht, Development und Operations zu vereinen.

\section{DevOps}

Webapplikationen werden zumeist von Entwickler erstellt und dann einem Administratoren-Team zur Installation übergeben. Teilweise wird die Applikation auch von den Entwicklern installiert und dann von den Administratoren langfristig gewartet. In größeren Unternehmen wird die Installation und Wartung teilweise auf zwei unterschiedliche Administratorenteams aufgeteilt.

Dies führt zu getrennten Teams mit getrennten Wissensstand und kann im worst-case auch zu Bunkerdenken --- us vs them --- führen. Diese Trennung behindert den Informationsfluss und verhindert, und führt künstliche Schranken im Verantwortlichkeitsgefühl ein ("die Admins sind dafür verantwortlich"). Im Fehlerfall führt dieses Bunkerdenken auch zum Herumschieben der Verantwortung zwischen Parteien.

DevOps versucht nun, wie der Name schon sagt, die Trennung von Entwicklung ("Development") und Administration ("Operations") zu beenden. Prinzipiell ist DevOps mehr eine Philosophie/gelebte Firmenkultur die stark von der Kultur der kontinuierlichen schritt-weisen Verbesserung (z. B. Kanban in Japan) geprägt ist. Bei der Umsetzung bindet es stärker Entwickler in klassische Operations-Bereiche wie Deployment ein.

Eine gute Beschreibung ist, dass DevOps \textit{combines agile IT development with agile delivery}.

\subsection{Agile Methoden}

Agile Methoden sind ein neueres Projektmanagement-Muster, welches im Agile Manifesto folgende Grundsätze definiert:

\begin{itemize}
	\item Individuals and interactions over processes and tools
	\item Working Software over comprehensive documentation
	\item Customer collaboration over contract negotiation
	\item Responding to change over following a plan
\end{itemize}

Umgesetzt führt dies zumeist dazu, dass monolithische Projekte in kleine minimale Teilprojekte/Schritte transformiert werden. Diese werden dann, in Reihenfolge der Kundenprioitisierung, abgearbeitet und regelmäßig der Fortschritt mit dem Kunden besprochen. Im Zuge des Projektes kommt es häufiger zu Änderungswünschen, diese können dann als weiteres Teilprojekt/Schritt inkludiert werden, die Geschwindigkeit des Teams kann über Projektdauer immer genauer eingeschätzt werden.

Damit die Projektsteuerung bei agiler Methodik funktioniert, darf ein einmal erledigter Schritt/Problem nicht immer wieder (durch Bugs) kosten verursachen. Aus diesem Grund wird hier stark auf automatisierte Tests gesetzt. Sofern diese Tests erfolgreich durchlaufen wird davon ausgegangen, dass das Produkt funktioniert. Der \textit{master} oder \textit{production}-Branch der Software sollte niemals fehlerhafte Testcases besitzen, kann daher jederzeit an den Kunden ausgeliefert werden.

\subsubsection{Anwendbarkeit agiler Methoden}

Agile Methoden sind natürlich nicht für alle Projekte geeignet und werden eher bei Startup-Projekten bzw. explorativen Projekten angetroffen. Bei Anwendungen mit hoher Sicherheitsrelevanz gibt es Zeitweise sehr genau aus-spezifizierte Lasten-/Pflichtenhefte, diese müssen dann auch dementsprechend umgesetzt werden.

\subsection{Infrastructure as Code}

Ein Grundsatz Agiler Methoden heißt \textit{Working Software over comprehensive documentation}.

Der Fokus auf \textit{Working Software} anstatt auf Dokumentation schlägt sich auch bei dem Deployment (dem Installieren der Software) nieder: dies wird zumeist automatisiert als Skript durchgeführt und nicht als Dokumentation ausgeliefert (und entspricht dadurch bereits dem DevOps-Gedanken).

Da im Zuge von Agilen Methoden versucht wird möglichst früh und möglichst häufig lauffähigen Code beim Kunden bereitzustellen (bzw. als Testservice dem Kunden zur Verfügung zu stellen) passieren Installationsvorgänge regelmäßig. Um hier nun Redundanzen zu vermeiden (bzw. um Konfigurationsfehler zu verhindern) wurden hier (historisch betrachtet) Installationsanweisungen immer stärker durch automatisierte Skripts ersetzt. Danach wurden dezidierte Deploymentstools (wie z. B. \textit{capistrano}) für das Setup der Applikation konfiguriert und verwendet. Im Laufe der Zeit wurden diese Tools nicht nur für die Applikation selbst, sondern auch für Datenbanken, Systemservices, etc. angewandt; die historische Evolution sind mittlerweile dezidierte Frameworks die zum Setup der Systeme dienen (wie z. B. Puppet, Chef oder Ansible).

Ein weiterer Vorteil dieses Ansatz ist, dass die verwendete Konfiguration innerhalb der (hoffentlich) verwendeten Source Code Versionierung automatisch versioniert und Veränderungen dokumentiert werden. Dies erlaubt das einfachere Debuggen von Regressionen.

\subsection{Continuous Integration and Continuous Delivery}

Die Verwendung von Tests zur Sicherung der Softwarequalität ist ein essentieller Bestandteil Agiler Methoden. Dem Kunden werden regelmäßig neue Programmversionen mit erweiterten Features zugestellt und anhand des Kundenwunsches neue Features selektiert und im nächsten Programmier-Sprint hinzugefügt. Würden Features fertiggestellt werden, die Fehler beinhalten und müsste man nachträglich diese Fehler immer wieder neu korrigieren, würde dadurch die Geschwindigkeit des Programmierteams stark leiden. Um dies zu verhindern, werden massiv Softwaretests geschrieben, die überprüfen ob die gewünschten Kundenfeatures ausreichend implementiert wurden. Diese Tests werden aufgerufen, bevor ein neues Feature in die, dem Kunden übermittelten, Version integriert wird. Auf diese Weise wird automatisch eine Kontrolle der Qualität durchgeführt.

Um sicher zu stellen, dass diese Tests auch wirklich aufgerufen werden, werden diese Tests automatisiert aufgerufen. Dies wird im Zuge des Continuous Integration Prozess durchgeführt: nach jeder Änderung wird versucht, die Software zu bauen (builden) und anschließend werden die vorhandenen Tests ausgeführt. Falls ein Fehler auftritt, wird der betroffene Entwickler sofort notifiziert.

Während Unit-Tests primär auf das Testen von Funktionen abzielen, werden auch statische Source Code Tests integriert. Diese überprüfen die Qualität des gelieferten Codes (z. B. Coding Guidelines) und sind n zweites Standbein automatisierter Tests.

In einem finalen Schritt kann auch Continuous Delivery angewandt werden, dies ist quasi die Ausdehnung des Continuous Integration Prozesses auf das installieren der fertigen Software (in einer Testumgebung oder, im Extremfall, direkt beim Kunden). Hier kann nach erfolgtem Bauen und Testen der Software diese auf Knopfdruck (oder vollautomatisiert) in einem Test- bzw. Produktivsystem eingespielt werden. Der Fluss von der Entwicklung über die Kontrolle bis zur Installation ist somit vollzogen. Falls dies alles durch den Entwickler konfiguriert wurde, kommt kein klassischer System-Administrator im Prozess vor. Damit gibt es keine Teilung der Kompetenzen und Verantwortung mehr, der Schritt zu DevOps ist vollzogen. 

\section{DevOps and Security}

Die Abkürzung DevOps besteht aus Development und Operations, das Wort Security kommt dabei nicht vor.

Das Problem, Sicherheit in DevOps zu integrieren ist ähnlich wie das ursprüngliche Admin/Developer-Problem das DevOps lösen sollte. Wenn die Sicherheitsverantwortlichen ein getrenntes Team, losgelöst vom DEvOps-Prozess, sind, dann ergibt sich wieder Bunkerdenken, Wissensinseln als auch ein fehlendes Verantwortungsgefühl.

Es wird nun Versucht das Sicherheitsteam in das DevOps-Team zu integrieren und dadurch Security als gemeinsame Verantwortung zu etablieren. Die Grundidee ist schön in folgendem Satz ausgeführt: \textit{Security is built into the System instead of being applied upon the finished product}.

Häufiger wird zwischen verschiedenen Ansätzen um Security einzubauen unterschieden:

\begin{itemize}
	\item DevOpsSec: es wird ein Produkt entwickelt, danach wird die Administration durchgeführt. Final wird Security gewährleistet: ein Beispiel dafür wäre es, dass nach Inbetriebnahme das Security-Team Security-Patches einspielt. Das klassische Beispiel wäre das Anpassen und der Betrieb von Standardsoftware.
	\item DevSecOps: zuerst wird entwickelt, danach wird Security betrachtet und danach die Administration fortgesetzt. Dieser Ansatz wird aktuell (2019) häufig gesehen und ist zumindest besser als die Security generell zu ignorieren. Ein Beispiel hierfür wäre es, die Inbetriebnahme von der erfolgreichen Durchführung einer Sicherheitsüberprüfung abhängig zu machen.
	\item SecDevOps: betrachtet initial die Security (z. B. schon während der Planung der Software). Dadurch durchdringt Security die gesamte Entwicklung als auch die Administration.
\end{itemize}

Security bewirkt meistens einen Mehraufwand für die Entwickler. Um diesen Mehraufwand zu begrenzen, wird auch hier (im DevOps-Spirit) stark auf Automatisierung gesetzt. So werden z. B. Sicherheitstests als automatisierte Testprogramme implementiert und während der Testphase innerhalb des Continuous Integration Prozesses ausgeführt. Dadurch werden die Sicherheitstests automatisch Teil des Abnahme-/Verifikationsprozess und durchdringt auf diese Weise die gesamte Entwicklung.

\subsection{Automatisierung}

Um die konsistente und regelmäßige Verwendung der Sicherheitstests zu enforcen wird die Ausführung der Sicherheitstests stark automatisiert. Dieses Kapitel führt häufig verwendete automatisierte Tests an:

\subsubsection{Automated Unit Tests}

Unit Tests sind minimale Tests die ein Feature verifizieren. Die meisten Software-Frameworks erlauben es, Tests auf Controller-Ebene zu schreiben für deren Ausführung die Applikation nicht als gesamtes gestartet werden muss.

Alternativ kann ein Unit-Test z. B. als einfaches Shellskript, Python-Requests2 skript oder als JUnit-Test unter Verwendung von Java-Bibliotheken durchgeführt werden. In dem Fall muss im Zuge der Tests ein Webserver gestartet werden, diese Tests sind daher zeitaufwendiger.

Integrations-Tests testen die Funktionalität des Gesamtsystems. Hierbei kann unter anderem auf Browser-basierte Frameworks wie Selenium oder Capybara zurückgegriffen werden. Durch diese wird ein oder mehrere Benutzer mittels eines virtuellen Browsers simuliert --- die Tests beschreiben die Benutzernavigation und -operationen innerhalb der Webseite. Hier kann man z. B. zwei Benutzer simulieren: ein Benutzer legt Daten an und ein zweiter Benutzer versucht (invalid) auf diese Daten zuzugreifen. Diese Tests sind um einiges Ressourcen- (da ein Webbrowser und Webserver benötigt wird) als auch zeitaufwendiger und werden daher zumeist nicht nach jeder Sourcecode-Änderung durchgeführt.

\subsubsection{Static Source Code Tests}

Analog zur Analyse der Qualität des Source Codes (während des normalen DevOps-Prozesses) können auch Tools zur statischen Analyse des Source Codes inkludiert werden. Diese prüfen den Source Code auf bekannte Programmiermuster und -fehler die sicherheitsrelevante Konsequenzen besitzen können.

Hier gibt es meistens Programmier- und Framework-abhängige Tools wie z. B. \textit{bandit} für \textit{Python}, \textit{Brakeman} für \textit{Ruby on Rails} und \textit{SpotBugs} für \textit{Java}. Eine Ebene über diesen Einzeltools funktioniert \textit{OWASP SonarCube}. Dieses Tool kann intern die gesamten erwähnten Subtools anwenden und besitzt auch Plugins um mittels \textit{OWAPS dependency-check} eine Überprüfung der verwendeten Abhängigkeiten (z. B. Bibliotheken) auf Schadcode hin durchzuführen.

\subsubsection{Dynamic Application Scans}

Zusätzlich zur statischen Source-Code Analyse kann man auch dynamische Scans verwenden. Hierbei wird zumeist die Applikation in einer Testumgebung (z. B. \textit{staging}) automatisiert installiert und danach mittels automatisierter Web Application Security Scanner gescripted ein Test durchgeführt. Im Falle einer Regression werden die Entwickler benachrichtigt. Beispiele hierfür wäre z. B. das automatisierte Scannen einer Applikation mittels \textit{OWASP ZAP} unter Zuhilfename des \textit{full-scan.py}-Skripts.

\section{Reflektionsfragen}

\begin{enumerate}
	\item Was ist der Grundgedanke dabei, DevOps und Security zu verbinden?
	\item Welche Sicherheitsmaßnahmen können im Zuge von SecDevOps automatisiert durchgeführt werden? Erläutere die jeweiligen Maßnahmen.
\end{enumerate}

\chapter{WAF: mod\_security}

\textit{mod\_security} ist eine Web-Application Firewall --- dies bedeutet, dass sie laut IOS/OSI Model auf Applikationsebene (Level 7) arbeitet und daher auch die Sprache der Applikation (in diesem Fall HTTP) spricht und versteht. Sie ist eine der bekanntesten Open-Source WAFs und wird daher im Zuge dieses Skripts betrachtet.

Laut dem eigenen Selbstverständnis bietet sie \textit{Real-Time Application Security Monitoring and Access Control}: ein- und ausgehende HTTP Anfragen und Antworten können in Echtzeit analysiert und aufgrund von Regeln kontrolliert (z. B. blockiert) werden. \textit{mod\_security} blockiert by Default keinen Traffic, alle Entscheidungen müssen explizit über Regeln konfiguriert werden. Ein häufig verwendeter Regelsatz ist de OWASP Core Rule Set (CRS). Die WAF führt kein Input-Sanititzing durch: ein Request wird also entweder akzeptiert oder blockiert. Dies war eine Design-Entscheidung des Entwicklers um sich auf eine Kernaufgabe zu konzentrieren und diese gut zu lösen (do one thing and do this well).

\textit{mod\_security} kann entweder als Apache Modul (daher der Name= innerhalb eines bestehenden Apache Webservers integriert werden, oder als eigenständiger Reverse-Proxy (als eigenständiger Apache-Server) vor mehrere Applikationsserver vorgeschalten werden.

Wie bereits erwähnt ist der primäre Einsatzzweck von \textit{mod\_security} ist das Monitoren und Kontrollieren von HTTP Traffic in Echtzeit und das Erstellen von Zugriffsentscheidungen aufgrund dieses Traffics. Konkreter kann z. B. \textit{Virtual Patching} durchgeführt werden. Dabei wird eine Sicherheitslücke gepatched ohne, dass die Web-Applikation selbst angepasst wird. Beispielsweise könnte es sich bei dem Sicherheitsproblem um einen 0day (ohne verfügbaren Patch), oder bei der Applikation um eine ungewartete proprietäre Applikation handeln. In diesem Fall würde das Angriffsmuster analysiert und für den Angriffsvektor eine Regel hinterlegt werden, welche bösartige Angriffe automatisiert erkennt und die jeweiligen Requests blockiert. Eine weitere Möglichkeit ist die Verwendung vordefinierter Regelwerker zum Hardening bestehender Applikationen (dadurch wird eine weitere Schutzschicht über eine eigentlich sichere Applikation eingezogen).

Eine unterschätzte Möglichkeit von \textit{mod\_security} ist der Einsatz als Log-Quelle. Webserver protokollieren im Normalfall nur die zugegriffene URL (also keinen Request-Body und auch keine Antwortdokumente). Mittels \textit{mod\_security} kann ein Administrator nun genau definieren, welche Requests in welcher Tiefe protokolliert werden sollten.

\section{Functionality and Transactions}

Grundsätzlich kann die implementierte Funktionalität in vier Bereiche aufgeteilt werden:

\begin{itemize}
	\item parsing: eingehende und ausgehene HTTP Nachrichten müssen analysiert werden.
	\item buffering: teilweise werden Nachrichten auf mehrere Teilnachrichten aufgeteilt. \textit{mod\_security} sammelt diese, fügt diese zu einer gemeinsamen Nachricht zusammen und ruft erst danach die Filterregeln auf.
	\item rule-engine: ist die aktive Komponente welche die bereitgestellten Regeln auf die gesammelten HTTP-Nachrichten anwendet.
	\item logging: führt das Logging durch.
\end{itemize}

Hier kann man auch schon den Overhead von \textit{mod\_security} erkennen: auf der einen Seite gibt es einen CPU-Overhead da alle Nachrichten analysiert werden müssen, auf der anderen Seite müssen für das Buffering alle ein- und aus-gehenden Nachrichten im Speicher gesammelt werden und führen so zu Speicher-Overhead. Falls zuviel Speicher verbraucht wird, müssen Nachrichten auf der Festplatte/SSD zwischengespeichert werden. Beides erhöht die Latenz-Zeit die ein Benutzer wahrnimmt.

Ein eingehender HTTP Request und die dazugehörende Response durchläuft innerhalb von \textit{mod\_security} folgende fünf Phasen:

\begin{enumerate}
	\item request headers : cheap, decide if you want to look at body content
	\item request body : think POST-Requests, attached Files
	\item response headers : cheap, decide if you want to look at body content
	\item response body, attached Files
	\item logging : decide if the request/response will be logged or not
\end{enumerate}

In Phase 1 und 2 kann das Weiterleiten des Requests zum Applikationsserver blockiert werden. Innerhalb der phase 3 und 4 kann das Antwortdokument zum Client blockiert werden, die Operation wird allerdings am Applikationsserver exekutiert (dies kann z. B. verwendet werden um Data-Extraktion zu verhindern). Inder Phase 5 kann ein Request nicht mehr blockiert werden.

\subsection{Beispielstransaktion}

Anhand der Debug-Logs von \textit{mod\_security} wird hier nochmal eine Transaktion erklärt.

Folgender Input Request mit jeweils einem Parameter in der URL (\textit{a}) und einem Parameter im Body (\textit{b}).

\begin{minted}{http}
POST /?a=test HTTP/1.0
Content-Type: application/x-www-form-urlencoded
Content-Length: 6

b=test
\end{minted}

Dies führt zu folgendem Antwortdokument mit der Nachricht \textit{Hello World!} im Content:

\begin{minted}{http}
HTTP/1.1 200 OK
Date: Sun, 17 Jan 2010 00:13:44 GMT
Server: Apache
Content-Length: 12
Connection: close
Content-Type: text/html

Hello World!
\end{minted}

Folgende Phasen können nun in den Log-Dateien erkannt werden:

\begin{minted}{text}
[4] Initialising transaction (txid SopXW38EAAE9YbLQ).
[5] Adding request argument (QUERY_STRING): name "a", value "test"
[4] Transaction context created (dcfg 8121800).
[4] Starting phase REQUEST_HEADERS.
\end{minted}

Für jeden eingehenden HTTP-Request wird zunächt eine eindeutige Transaktions-ID vergeben. Anschließend wird der HTTP Header analysiert und der Parameter \textit{a} extrahiert. Abschließend werden die Regeln der Phase 1 (Request-Header) angewandt.

\begin{minted}{text}
[4] Second phase starting (dcfg 8121800).
[4] Input filter: Reading request body.
[9] Input filter: Bucket type HEAP contains 6 bytes.
[9] Input filter: Bucket type EOS contains 0 bytes.
[5] Adding request argument (BODY): name "b", value "test"
[4] Input filter: Completed receiving request body (length 6).
[4] Starting phase REQUEST_BODY.
\end{minted}

Falls die Nachricht noch nicht blockiert wurde, wird nun der Content des Requests analysiert und dabei der Parameter \textit{b} extrahiert. Jetzt besitzt \textit{mod\_security} Zugriff auf alle Parameter und die Regeln der Phase 2 (Request Body) werden angewendet.

\begin{minted}{text}
[4] Hook insert_filter: Adding input forwarding filter (r 81d0588).
[4] Hook insert_filter: Adding output filter (r 81d0588).

[4] Input filter: Forwarding input: mode=0, block=0, nbytes=8192 (f 81d2228, r 81d0588).
[4] Input filter: Forwarded 6 bytes.
[4] Input filter: Sent EOS.
[4] Input filter: Input forwarding complete.
\end{minted}

Wenn auch diese Regeln den Request nicht verwerfen, wird der Request an den Applikationsserver/die Applikation weitergeleitet. \textit{mod\_security} kann nun abwarten, bis ein entsprechendes Antwortdokument empfangen wird.

\begin{minted}{text}
[9] Output filter: Receiving output (f 81d2258, r 81d0588).
[4] Starting phase RESPONSE_HEADERS.

[9] Output filter: Bucket type MMAP contains 12 bytes.
[9] Output filter: Bucket type EOS contains 0 bytes.
[4] Output filter: Completed receiving response body (buffered full - 12 bytes).
[4] Starting phase RESPONSE_BODY.
\end{minted}

Analog zu der Request-Phase wird nun das Antwortdokument analysiert und die Regeln der jeweiligen Phasen (3 und 4) angewendet. Hier sieht man auch schön, dass auch das Antwortdokument gespeichert wird --- werden z. B. viele große Dokumente zugestellt (oder Dateien heruntergeladen) müssen diese zwischengespeichert, und auf diese Weise serverseitig viele Ressourcen verbraucht, werden.

\begin{minted}{text}
[4] Output filter: Output forwarding complete.
[4] Initialising logging.
[4] Starting phase LOGGING.
[4] Audit log: Ignoring a non-relevant request.
\end{minted}

Abschließend wird noch die Log-Phase durchspielt und aufgrund dieser erkannt, ob der Request bzw. die Response in die Logdatei aufgenommen werden sollte.

\section{Rules}

Das Herz einer \textit{mod\_security} Installation sind die verwendeten Filterregeln. Diese besitzen immer das idente Format:

\begin{minted}{text}
SecRule ARGS OPERATOR ACTIONS
\end{minted}

Die jeweiligen Bereiche sind einfacher erklärt:

\begin{description}
	\item[ARGS]: beschreibt auf welchen Bereich (z. B. auf welchen Parameter) eine jeweilige Operation angewendet werden soll.
	\item[OPERATOR]: definiert die Abfrage/den Filter der auf die ARGS angewandt werden soll.
	\item[ACTIONS]: beschreibt die Aktivitäten (z. B. blockieren und loggen) die angewandt werden, falls der Operator erfolgreich auf die Argumente angewandt werden konnte.
\end{description}

\subsection{Rules: Args}

Beispiele für mögliche Argumente:

\begin{itemize}
	\item ARGS (GET+POST arguments)
	\item ARGS\_GET, ARGS\_POST: die Parameter dre Header/des Contents
	\item FILES: hochgeladene Dateien
	\item FULL\_REQUEST: der gesamte Request
	\item QUERY\_STRING, REQUEST\_BODY: raw data
	\item REQUEST\_HEADERS: Header Werte
	\item REQUEST\_METHOD: die Verwendete HTTP Methode (POST/GET)
	\item REQUEST\_URI
	\item REQUEST\_LINE
	\item REMOTE\_ADDRESS: die Adresse des zugreifenden Clients
\end{itemize}

\subsubsection{Operatoren}

Operatoren beginnen meistens mit einem @-Symbol, einige Beispiele:

\begin{itemize}
	\item "@contains <script >": überprüft ob in den Arguments ein script-Tag vorkommt
	\item "@detectSQLi": überprüft die Arguments auf SQL-Injection-Muster
	\item "@detectXSS": überprüft die Arguments auf XSS-Muster
	\item "@inspectFile /path/to/util/runav.pl": die Arguments werden an das angeführte Skripts übergeben, das Ergebnis des Operators ist das Ergebnis des Skripts. Auf diese Weise kann z. B. jedes hochgeladene File mittels eines Virenscanners überprüft werden, bevor dieses File an den Applikationsserver übergeben wird.
	\item "@ipMatch 192.168.1.100": Vergleich auf eine IP-Adresse
	\item "@validateDTD /path/to/xml.dtd": überprüft ob das übergebene Argument einem definierten XML-Schema entspricht.
\end{itemize}

\subsubsection{Actions}

Actions definieren die Operationen, die exekutiert werden sollten falls die Operatoren erfolgreich angewandt werden konnten. Diese Operatoren können lt. \textit{mod\_security} Dokumentation in verschiedene Bereiche eingeteilt werden:

\begin{itemize}
	\item disruptive: verändern den Datenfluss, z. B. \textit{allow} oder \textit{deny}
	\item flow: beziehen sich auf die Regeln selbst, z. B. \textit{skip} oder \textit{chain}
	\item meta-data: dienen zur Dokumentation
	\item variable: für komplexe Regeln können Variablen gesetzt bzw. ausgelesen werden
	\item logging: Definition der Daten, die geloggt werden sollten
	\item special
	\item miscellaneous: everything else
\end{itemize}

\subsubsection{Beispiele}

Ein einfaches primitives Beispiel:

\begin{minted}{text}
SecRule ARGS "@contains <script>" log,deny,status:404
\end{minted}

Diese Regeln verwendet als Datenquelle alle extrahierten Argumente (\textit{ARGS}) und überprüft, ob in diesen ein HTML script-Tag vorkommt. In dem Fall wird die Anfrage mit einem 404 Fehler blockiert und geloggt.

Eine komplexere Regel:

\begin{minted}{text}
SecRule REQUEST\_LINE|REQUEST\_HEADERS|REQUEST\_HEADERS\_NAMES "@contains () \{" "id:420008,phase:2,t:none,t:lowercase,deny,status:500,log,msg:'Malware expert - user-agent: Bash ENV Variable Injection Attack'"
\end{minted}

Diese Regel sucht in verschiedenen Datenquellen nach dem String \textit{() \{}. Dieser String beschreibt einen Bash-Funktionsaufruf und sollte in regulärem Verkehr nicht vorkommen. Falls dieser String gefunden wird, wird der Request mit einem 500er Statuscode abgelehnt und protokolliert.

\begin{minted}{text}
SecRule REMOTE\_ADDR "@ipMatch 192.168.1.101" id:102,phase:1,t:none,nolog,pass,ctl:ruleEngine:off
\end{minted}

Diese Regel würde Verkehr von der IP-Adresse 192.168.1.101 immer akzeptieren (durch ruleEngine:off werden keine Folgeregeln mehr verwendet), diese Requests werden ebenso nicht geloggt. Dadurch wird ein Whitelisting der IP-Adresse durchgeführt.

\begin{minted}{text}
SecRule ARGS:username “@streq admin” chain,deny
SecRule REMOTE\_ADDR “!streq 192.168.1.111”
\end{minted}

Das letzte Beispiel zeigt, wie zwei Regeln mittels \textit{chain} verknüpft werden. Es wird zuerst die erste Regel angewendet (\textit{für das Argument username wurde der Wert admin verwendet}) und dann mit der Regel \textit{die IP-Adresse ist nicht 192.168.1.111} kombiniert. Falls beide Regeln wahr sind, wird die Anfrage abgelehnt. Dies entspricht also der Regel \textit{Ein Administrator darf sich nur ausgehend von der IP-Adresse 192.168.1.111 einloggen}.

\subsection{Log format}

Das \textit{mod\_security} Format entspricht nicht 100\% anderen bekannten Log-Formaten. Eine protokollierte Transaktion wird über mehrere Sublogs beschrieben. Jedes Sublog beginnt mit einem MIME-ähnlichem Header, dieser besteht aus einer eindeutigen ID über welchen die Sublogs aggregiert werden können (in dem Beispiel \textit{f9adec1d}) und einer Log-Kategorie (dies ist der angehängte Buchstabe).

\begin{minted}{text}
--f9adec1d-A--
[25/Sep/2016:18:42:50 +0300] V@fwen8AAQEAAA2TDbQAAAAK 127.0.0.1 35965 127.0.0.1 443
--f9adec1d-B--
GET / HTTP/1.1
Host: malware.expert
Accept: */*
User-Agent: () {

--f9adec1d-F--
HTTP/1.1 500 Internal Server Error
Content-Length: 613
Connection: close
Content-Type: text/html; charset=iso-8859-1

--f9adec1d-E--

--f9adec1d-H--
Message: Access denied with code 500 (phase 2). String match "() {" at REQUEST_HEADERS:User-Agent. [file "/etc/modsecurity/malware_expert.conf"] [line "97"] [id "420008"] [msg "Malware expert - user-agent: Bash ENV Variable Injection Attack"]
Action: Intercepted (phase 2)
Stopwatch: 1474818170070960 1010 (- - -)
Stopwatch2: 1474818170070960 1010; combined=177, p1=25, p2=147, p3=0, p4=0, p5=5, sr=55, sw=0, l=0, gc=0
Response-Body-Transformed: Dechunked
Producer: ModSecurity for Apache/2.9.0 (http://www.modsecurity.org/).
Server: Apache
Engine-Mode: "ENABLED"

--f9adec1d-Z--
\end{minted}

In Abhängigkeit der Transkation können unterschiedliche Log-Kategorien befüllt werden. \textit{A} beinhaltet immer den Zeitstempel und die Adressen der betroffenen Kommunikationspartner, \textit{B} ist der eingehende Request, \textit{F} die erzeugte Antwort; \textit{H} beschreibt die Regel die angewandt wurde.

Leider ist es nicht so einfach diese Informationen mittels \textit{grep} zu parsen.

\section{OWASP Core Ruleset}

Es macht natürlich wenig Sinn, für jede \textit{mod\_security}-Installation getrennte Regelwerke für common Angriffsvektoren zu erstellen. Aus diesem Grund haben sich Default-Regelwerke gebildet die als Initiale Regeln eingespielt werden können. Das bekannteste Beispiel hierfür ist das \textit{OWASP Core Rule Set} (auch CRS genannt), die aktuelle Version ist 3, dieses Regelwerk wird gerne zum Hardening von Webservern verwendet.

Dieses Ruleset beinhaltet Regeln welche die Angriffsvektoren der OWASP Top 10 (als auch weitere Angriffsvektoren) abdecken. Um dieses Regelwerk zu aktivieren, lädt der Administrator den Regelsatz als auch ein Konfigurations-File (\textit{crs-setup.conf}) vom OWASP-Server herunter und inkludiert diese innerhalb der Konfiguration des Apache Webservers. Dabei kann der Admin entweder alle Regeln inkludieren oder nur einzelne Subbereiche (z. B. nur Regeln für SQL-Injection Angriffe).

Ein häufiges Problem von Web Application Firewalls ist, dass valide Anfragen als Schadcode erkannt werden (diese Fehler werden auch als \textit{false positives} bezeichnet). Eine hohe Rate an false positives zerstört die Benutzerakzeptanz und führt im Normalfall dazu, das die WAF aus Akzeptanzgründen wieder deaktiviert wird. Um dem entgegenzuwirken besitzt das OWASP Core Rule Set eine Paranoia-Einstellung welche von 1 (default) bis 4 gesetzt werden kann. Je höher das Paranoia-Level umso mehr Schadcodes werden erkannt, allerdings steigt damit auch das Risiko von False-Positives. In Abhängigkeit der Gefährdung der Applikation kann nun ein geeignetes Paranoia-Level gewählt werden.

\section{Reflektionsfragen}

\begin{enumerate}
	\item Was versteht man unter einer Web-Application Firewall? Welche Einsatzzwecke bzw. Verwendungsmöglichkeitne gibt es für solche?
	\item Aus welchen Teilen besteht eine mod\_security Regel? Erläutere die jweiligen Teile.
	\item Was wird durch folgende Regel: \textit{SecRule ARGS "@contains <script>" log,deny,status:404} durchgeführt?
\end{enumerate}

\begin{enumerate}
	\item Was ist der Grundgedanke dabei, DevOps und Security zu verbinden?
	\item Welche Sicherheitsmaßnahmen können im Zuge von SecDevOps automatisiert durchgeführt werden? Erläutere die jeweiligen Maßnahmen.
	\item Was sind Docker Registries? Welchen negativen Einfluss auf die Sicherheit können diese nehmen?
	\item Auf welche Best-Practises sollte beim Bau von Container geachtet werden?
	\item Welche grundlegenden Sicherheitsprobleme gibt es bei Docker (bzw. bei Containerlösungen)? Erläutere vier davon.
\end{enumerate}
